# -*- coding: utf-8 -*-
"""analitica 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UvcbRiox5l-4CAPpqWVvZk_Rjh8bkAuy

# **Trabajo 01: Modelos de riesgo de crédito**

# Integrantes
-Camilo Andres Granada Mejia

-Alejandro Zapata Quintero

# Importacion de datos
"""

import pandas as pd
import csv
import numpy as np
from google.colab import files
import io
import matplotlib.pyplot as plt
from sklearn import preprocessing
from matplotlib import colors
import seaborn as sb
from scipy import stats
uploaded = files.upload()
dataframe = pd.read_csv(io.BytesIO(uploaded['dataset.csv']))

"""# Comportamiento de datos en el dataset"""

dataframe.describe()

dataframe.head()

"""# Se verifica la existencia de nulos"""

dataframe.isnull().sum()

"""# Se eliminan los nulos"""

dataframe.dropna(axis=0,inplace=True)
dataframe.isnull().sum()

"""# Nuevo comportamiento sin nulos"""

dataframe.describe()

"""# Se revisan las variables con comportamientos atipicos"""

verti = dataframe['person_age'].value_counts().values
hori = dataframe['person_age'].value_counts().index
fig = plt.figure(figsize = (15, 5))
plt.bar(hori, verti)
plt.title('person age')
verti = dataframe['person_emp_length'].value_counts().values
hori = dataframe['person_emp_length'].value_counts().index
fig = plt.figure(figsize = (15, 5))
plt.bar(hori, verti)
plt.title('person emp length')

"""# Se limita la informacion para eliminar datos atipicos"""

dataframe = dataframe.drop(dataframe[dataframe['person_age'] > 80].index, axis=0)
dataframe = dataframe.drop(dataframe[dataframe['person_emp_length'] > 40].index, axis=0)
dataframe.describe()

"""# Nueva distribucion de los datos filtrados"""

verti = dataframe['person_age'].value_counts().values
hori = dataframe['person_age'].value_counts().index
fig = plt.figure(figsize = (15, 5))
plt.bar(hori, verti)
plt.title('person age')
verti = dataframe['person_emp_length'].value_counts().values
hori = dataframe['person_emp_length'].value_counts().index
fig = plt.figure(figsize = (15, 5))
plt.bar(hori, verti)
plt.title('person emp length')

dataframe2 = dataframe[['person_age', 'person_income','person_emp_length','loan_amnt','loan_int_rate','loan_status','loan_percent_income','cb_person_cred_hist_length']].copy()
dataframe2.head()

"""# Funcion para calcular el Woe (la continua no funciona bien)

"""

# function to calculate WoE and IV of categorical features
# The function takes 3 arguments: a dataframe (X_train_prepr), a string (column name), and a dataframe (y_train_prepr).
def woe_discrete(df, cat_variabe_name, y_df):
    df = pd.concat([df[cat_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    df = df.sort_values(['WoE'])
    df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

def woe_ordered_continuous(df, continuous_variabe_name, y_df):
    df = pd.concat([df[continuous_variabe_name], y_df], axis = 1)
    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),
                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)
    df = df.iloc[:, [0, 1, 3]]
    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']
    df['prop_n_obs'] = df['n_obs'] / df['n_obs'].sum()
    df['n_good'] = df['prop_good'] * df['n_obs']
    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']
    df['prop_n_good'] = df['n_good'] / df['n_good'].sum()
    df['prop_n_bad'] = df['n_bad'] / df['n_bad'].sum()
    df['WoE'] = np.log(df['prop_n_good'] / df['prop_n_bad'])
    #df = df.sort_values(['WoE'])
    #df = df.reset_index(drop = True)
    df['diff_prop_good'] = df['prop_good'].diff().abs()
    df['diff_WoE'] = df['WoE'].diff().abs()
    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']
    df['IV'] = df['IV'].sum()
    return df

"""# Aplicacion del woe a las variables categoricas"""

woe_discrete(dataframe, 'person_home_ownership', dataframe2)

woe_discrete(dataframe, 'person_home_ownership', dataframe2)['IV'].sum()

woe_discrete(dataframe, 'loan_intent', dataframe2)

woe_discrete(dataframe, 'loan_intent', dataframe2)['IV'].sum()

woe_discrete(dataframe, 'loan_grade', dataframe2)

woe_discrete(dataframe, 'cb_person_default_on_file', dataframe2)

"""# Aplicacion de prueba anova a las variables continuas en grupos de 4"""

fvalue, pvalue = stats.f_oneway(dataframe['person_income'], dataframe['person_age'],  dataframe['person_emp_length'], dataframe['loan_amnt'])
print(fvalue, pvalue)

fvalue, pvalue = stats.f_oneway(dataframe['loan_int_rate'], dataframe['loan_status'], dataframe['loan_percent_income'], dataframe['cb_person_cred_hist_length'])
print(fvalue, pvalue)

"""# Se crean crosstabs para usar chi2 con las variables categoricas"""

crosstab7 = pd.crosstab(dataframe['loan_status'], dataframe['loan_intent'])
crosstab8 = pd.crosstab(dataframe['loan_status'], dataframe['loan_grade'])
crosstab9 = pd.crosstab(dataframe['loan_status'], dataframe['person_home_ownership'])
crosstab10 = pd.crosstab(dataframe['loan_status'], dataframe['cb_person_default_on_file'])

stats.chi2_contingency(crosstab7)

stats.chi2_contingency(crosstab8)

stats.chi2_contingency(crosstab9)

stats.chi2_contingency(crosstab10)

"""# Diagramas de pares para revisar comportamiento"""

sb.pairplot(dataframe,hue="loan_status")

"""# Matriz de correlacion"""

colormap = plt.cm.viridis
plt.figure(figsize=(10,10))
plt.title('Pearson Correlation of Features')
sb.heatmap(dataframe2.astype(float).corr(),
           vmax=1.0,
           cmap=colormap,
           annot=True,
           linewidths=0.1,
           linecolor='white',
           square=True)

"""# **Desarrollo del modelo**
# Se crean categorias para las variables edad, ingresos y monto prestado y se calculan tasas que relacionan otras variables
"""

dataframe['age_group'] = pd.cut(dataframe['person_age'], bins=[20, 26, 36, 46, 56, 66], labels=['20-25', '26-35', '36-45', '46-55', '56-65'])
dataframe['income_group'] = pd.cut(dataframe['person_income'], bins=[0, 25000, 50000, 75000, 100000, float('inf')], labels=['Bajo', 'Medio-Bajo', 'Medio', 'Medio-Alto', 'Alto'])
dataframe['loan_amount_group'] = pd.cut(dataframe['loan_amnt'], bins=[0, 5000, 10000, 15000, float('inf')], labels=['Pequeño', 'Mediano', 'Grande', 'Muy Grande'])
# Ratio Prestamo-ingresos
dataframe['loan_to_income_ratio'] = dataframe['loan_amnt'] / dataframe['person_income']

# Ratio prestamo-tiempo de empleo
dataframe['loan_to_emp_length_ratio'] =  dataframe['person_emp_length']/ dataframe['loan_amnt']

# Ratio Tasa de interes - Prestamo
dataframe['int_rate_to_loan_amt_ratio'] = dataframe['loan_int_rate'] / dataframe['loan_amnt']

dataframe.head()

"""# Se reordena el dataset"""

col_list = ['person_age',#
 'person_income',#
 'person_home_ownership',#
 'person_emp_length',#
 'loan_intent', #
 'loan_grade',#
 'loan_amnt',#
 'loan_int_rate',#
 'loan_status',#
 'loan_percent_income',#
 'cb_person_default_on_file',#
 'cb_person_cred_hist_length',
'age_group','income_group','loan_amount_group']

drop_colums = []
scale_cols = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income','loan_to_income_ratio', 'loan_to_emp_length_ratio',
       'int_rate_to_loan_amt_ratio']
ohe_colums = ['cb_person_default_on_file','loan_grade', 'person_home_ownership','loan_intent','income_group','age_group','loan_amount_group']
le_colums = []
dataframe = dataframe.drop(drop_colums, axis=1)

"""# Creacion de los sets de entrenamiento y prueba"""

X = dataframe.drop(['loan_status'], axis=1)
Y = dataframe['loan_status']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=12)
x_train.reset_index(inplace = True)
x_test.reset_index(inplace = True)
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
ohe.fit(x_train[ohe_colums])
merge_ohe_col = np.concatenate((ohe.categories_[0], ohe.categories_[1], ohe.categories_[2], ohe.categories_[3], ohe.categories_[4], ohe.categories_[5], ohe.categories_[6],))
merge_ohe_col
ohe_data = pd.DataFrame(ohe.transform(x_train[ohe_colums]).toarray(), columns=merge_ohe_col)
ohe_data2 = pd.DataFrame(ohe.transform(x_test[ohe_colums]).toarray(),columns=merge_ohe_col)
X_new = pd.concat([ohe_data, x_train], axis=1)
X_new = X_new.drop(ohe_colums, axis=1)
X_new_test = pd.concat([ohe_data2, x_test], axis=1)
X_new_test = X_new_test.drop(ohe_colums, axis=1)

from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler
scale_cols = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income', 'loan_to_emp_length_ratio', 'int_rate_to_loan_amt_ratio']
uniform_col= []
normal_col = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income', 'loan_to_emp_length_ratio', 'int_rate_to_loan_amt_ratio']
bimodal_col = []
scaler_uniform = MinMaxScaler()
scaler_normal = StandardScaler()
X_new.loc[:,normal_col] = scaler_normal.fit_transform(X_new.loc[:,normal_col])
X_new_test.loc[:,normal_col] = scaler_normal.transform(X_new_test.loc[:,normal_col])
scaler_bimodal = RobustScaler()

"""# Instalacion de librerias no disponibles, ignorar"""

import sys
ENV_COLAB = 'google.colab' in sys.modules

if ENV_COLAB:

    !pip install catboost
    !pip install ipywidgets
    !jupyter nbextension enable --py widgetsnbextension

# Commented out IPython magic to ensure Python compatibility.
# %pip install bayesian-optimization==1.4.1

"""# importacion y prueba de regresiones"""

from sklearn.linear_model import LinearRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
import lightgbm as lgb
from lightgbm import LGBMRegressor
from sklearn.metrics import make_scorer, mean_absolute_error
from sklearn.metrics import mean_squared_error as MSE
from hyperopt import hp, fmin, tpe
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from bayes_opt import BayesianOptimization
from sklearn.model_selection import KFold, cross_val_score
from lightgbm import LGBMClassifier
#estas son todas las regresiones disponibles
svc = SVC()
knc = KNeighborsClassifier() #algorithm='ball_tree', leaf_size=10, n_neighbors=18, p=1, weights='distance'
mnb = MultinomialNB()
dtc = DecisionTreeClassifier()
lrc = LogisticRegression()
rfc = RandomForestClassifier()
abc = AdaBoostClassifier()
bc = BaggingClassifier()
etc = ExtraTreesClassifier()
gbdt = GradientBoostingClassifier()
xgb = XGBClassifier()
cat = CatBoostClassifier(verbose=0)
lgb = lgb.LGBMClassifier()
clf = {
    'KN' : knc,
    'xgb':xgb,
    'cat':cat,
    'lrc':lrc
}
from sklearn.metrics import precision_score, accuracy_score, recall_score, confusion_matrix

def train_classifier(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    # Calculate confusion matrix to get TN and FP for specificity
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    specificity = tn / (tn + fp)

    return accuracy, precision, recall, specificity
X_new = X_new.drop(columns=[col for col in X_new.columns if pd.isna(col)], axis=1)
X_new_test = X_new_test.drop(columns=[col for col in X_new_test.columns if pd.isna(col)], axis=1)
accuracy_scores = []
precision_scores = []
recall_scores = []
specificity_scores = []

for name,clf in clf.items():

    current_accuracy,current_precision, current_recall, current_specificity = train_classifier(clf, X_new,y_train,X_new_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)
    print("Recall - ",current_recall)
    print("Specificity - ",current_specificity)
    print()

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)
    recall_scores.append(current_recall)
    specificity_scores.append(current_specificity)

"""# Regresion de los datos y uso de arboles de decision"""

from sklearn.ensemble import VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier


knn = KNeighborsClassifier()
cat = CatBoostClassifier(verbose=0)
lgb = LGBMClassifier()


ensemble = VotingClassifier(estimators=[('knn', knn),  ('cat', cat), ('lgb', lgb)], voting='soft',verbose=0)
current_accuracy,current_precision, current_recall, current_specificity = train_classifier(ensemble, X_new, y_train, X_new_test, y_test)

print("For ensemble")
print("Accuracy - ",current_accuracy)
print("Precision - ",current_precision)
print("Recall - ",current_recall)
print("Specificity - ",current_specificity)
print("-----------------------------------------------------------------------------------------------")
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor()
from sklearn.feature_selection import RFE

clf = RandomForestRegressor()


clf.fit(X_new,y_train)

feature_scores = pd.Series(clf.feature_importances_, index=X_new.columns).sort_values(ascending=False)
feature_scores

!pip install autogluon

"""# Creacion del modelo ML de autogluon"""

from sklearn.model_selection import train_test_split

#split dataframe into train and test sets
train, test = train_test_split(dataframe, test_size=0.2,random_state=12)

from autogluon.tabular import TabularDataset, TabularPredictor

predictor = TabularPredictor(label='loan_status').fit(train_data=train)
predictions = predictor.predict(test)
predictor.evaluate(test, silent=True)
predictor.leaderboard(test, silent=True)

"""# Definicion del score"""

def score(edad,home,income):
  score=0
  if edad<22:
    score=score+100
  elif edad>=22 and edad<26:
    score=score+120
  elif edad>=26 and edad<29:
    score=score+185
  elif edad>=29 and edad<32:
    score=score+200
  elif edad>=32 and edad<37:
    score=score+210
  elif edad>=37 and edad<42:
    score=score+225
  else:
    score=score+250
  if home=='OWN':
    score=score+225
  elif home=='RENT':
    score=score+110
  else:
    score=score+0
  if income<10000:
    score=score+120
  elif income>=10000 and income<17000:
    score=score+140
  elif income>=17000 and income<28000:
    score=score+180
  elif income>=28000 and income<35000:
    score=score+200
  elif income>=35000 and income<42000:
    score=score+225
  elif income>=42000 and income<58000:
    score=score+230
  else:
    score=score+260
  return score

"""# Implementacion del score en el dataset"""

dataframe['Score']=dataframe.apply(lambda row : score(row.person_age, row.person_home_ownership, row.person_income), axis=1)
dataframe.head()